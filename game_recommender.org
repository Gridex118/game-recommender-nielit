#+Author: Alex Rosegrid
#+Title: Game Recommender System as a Classification Problem
#+Startup: indent
#+OPTIONS: ^:nil


* Org, Jupyter Compatibility
First, activate the environment that has a jupyter installation. Then load the =emacs/jupyter= package.
#+begin_src emacs-lisp :results none
  (myPython/activate-conda-env "~/.opt/miniconda3/envs/nielit-project")
#+end_src
This can be used to convert the =org= file into a =ipynb= file.
#+begin_src shell :results none
  pandoc game_recommender.org -o game_recommender.ipynb
#+end_src

* Common Imports
#+begin_src jupyter-python :results none :noweb-ref common_imports
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns
#+end_src

* The Report

** The Problem Statement
#+begin_quote
Given a list of games previously played by a user, recommend new games.
#+end_quote

** The Data Set
The dataset was obtained from [[https://www.kaggle.com/datasets/antonkozyriev/game-recommendations-on-steam?select=games.csv][Kaggle]]. It comprises of 4 files,
  - games.csv :: A record of games. Here, we are mainly interested in the title, ratings, and the final
    price.
  - games_metadata.json :: Provides the tags/genres for the games. This can be used to find similarities
    between games.
  - users.csv :: A list of users, the number of games they bought, and the number of times they reviewed
    anything. The number of games bought can be used to filter out a number of users from being considered.
  - recommendations.csv :: List of user reviews, in a form that answers if said user recommends a certain
    game or not.

** Data Cleaning
:PROPERTIES:
:header-args:jupyter-python: :tangle game_recommender_cleaning.py :session cleaning
:END:

#+begin_src jupyter-python :results none :noweb yes
  <<common_imports>>
  import json
#+end_src

It is worth noting that these files, =users= and =recommendations= in particular, are much too large. Quite a
few fields are of no use to us.

*** Game Metadata
The original metadata file was badly formatted (set of top-level objects), so we use =jq= to fix
it. Additionally, we drop the game description.
#+begin_src shell
  jq -s 'del(.[].description)' games_metadata.json > games_metadata_slurp.json
  printf "%s Entries" $(jq '.[].app_id' games_metadata_slurp.json| wc -l)
#+end_src
#+RESULTS:
: 50872 Entries

From here, filter out the games that have no tags listed, since these won't really help in the classification.
#+begin_src shell
  jq '.[] | select(.tags | length > 0)' games_metadata_slurp.json| jq -s > games_metadata_processed.json
  printf "%s Entries" $(jq '.[].app_id' games_metadata_processed.json| wc -l)
#+end_src
#+RESULTS:
: 49628 Entries

We remove a few of these entries,
#+begin_src jupyter-python :results none
  with open("games_metadata_processed.json") as source:
      games_metadata = json.load(source)

  def get_reduced_games_metadata(games_metadata):
      LIMIT = 30000
      return games_metadata[:LIMIT]

  games_metadata = get_reduced_games_metadata(games_metadata)
#+end_src

Then save these to disk.
#+begin_src jupyter-python :results none
  with open("games_metadata_processed.json", "w") as sink:
      json.dump(games_metadata, sink)
#+end_src

*** Games
#+begin_src jupyter-python
  games = pd.read_csv("games.csv")
  print(games.head())
#+end_src

#+RESULTS:
#+begin_example
     app_id                              title date_release   win    mac  linux  \
  0   13500  Prince of Persia: Warrior Within™   2008-11-21  True  False  False   
  1   22364            BRINK: Agents of Change   2011-08-03  True  False  False   
  2  113020       Monaco: What's Yours Is Mine   2013-04-24  True   True   True   
  3  226560                 Escape Dead Island   2014-11-18  True  False  False   
  4  249050            Dungeon of the ENDLESS™   2014-10-27  True   True  False   

            rating  positive_ratio  user_reviews  price_final  price_original  \
  0  Very Positive              84          2199         9.99            9.99   
  1       Positive              85            21         2.99            2.99   
  2  Very Positive              92          3722        14.99           14.99   
  3          Mixed              61           873        14.99           14.99   
  4  Very Positive              88          8784        11.99           11.99   

     discount  steam_deck  
  0       0.0        True  
  1       0.0        True  
  2       0.0        True  
  3       0.0        True  
  4       0.0        True  
#+end_example

We are not concerned with the release dates, discounts or the original prices, only the final price of the
game; we can safely drop the corresponding fields. We also have no use for the user review count here (will
rely on the =recommendations= dataset) or the positive ratio (the rating sentament value suffices).
#+begin_src jupyter-python :results none
  games = games.drop(["date_release", "price_original", "discount", "user_reviews", "positive_ratio"], axis=1)
#+end_src

It's pobably a lot more efficient to just target a single platform, so let's choose the one with the largest
number of games compatible.
#+begin_src jupyter-python
  def plot_games_in_platform():
      platforms: list[str] = ["win", "steam_deck", "mac", "linux"]
      games_in_platform = games[platforms].sum()
      plt.bar(games_in_platform.keys(), games_in_platform, color=sns.color_palette("pastel"))
      plt.title("Number of games compatible with each platform")
      plt.xlabel("Platform")
      plt.ylabel("Compatible games")
      plt.show()

  plot_games_in_platform()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/33989c75f87303d0b891bbd95e72fea1283788f1.png]]

We note that the Stem Deck has the best compatibility, so, for simplicity's sake we assume that every user
owns one, and we can drop the other 3.
#+begin_src jupyter-python :results none
  games = games.drop(["win", "linux", "mac"], axis=1)
#+end_src
We also drop the games that are not compatible with the Steam Deck.
#+begin_src jupyter-python :results none
  games["steam_deck"] = games["steam_deck"].replace(False, np.nan)
  games = games.dropna(subset=["steam_deck"])

  # We are done with the filtering so, this column too is unneeded
  games = games.drop(["steam_deck"], axis=1)
#+end_src

Then we remove the games we filtered out in the games metadata dataset.
#+begin_src jupyter-python
  games_metadata = pd.read_json("games_metadata_processed.json")

  games = games[
      games["app_id"].isin(games_metadata["app_id"])
  ]

  print(f"{games.shape[0]} Entries")
#+end_src

#+RESULTS:
: 30000 Entries

Finally, we write this file to disk for future use.
#+begin_src jupyter-python :results none
  games.to_csv("games_processed.csv", index=False)
#+end_src

*** Users
#+begin_src jupyter-python
  users = pd.read_csv("users.csv")
  print(users.head())
  print(f"\n{users.shape[0]} rows")
#+end_src

#+RESULTS:
:     user_id  products  reviews
: 0   7360263       359        0
: 1  14020781       156        1
: 2   8762579       329        4
: 3   4820647       176        4
: 4   5167327        98        2
: 
: 14306064 rows

Inspecting the dataset, a number of users never bought any products,
#+begin_src jupyter-python
  users["products"] = users["products"].replace(0, np.nan)
  users["products"].isna().sum()
#+end_src
#+RESULTS:
: np.int64(139318)
We can ignore them since they do not help in making any predictions.
#+begin_src jupyter-python
  users = users.dropna(subset=["products"])
  print(f"{users.shape[0]} rows")
#+end_src
#+RESULTS:
: 14166746 rows

This is still a very large dataset, and needs to be reduced.

Using a scatterplot
#+begin_src jupyter-python
  sns.scatterplot(users, x="products", y="reviews")
  plt.title("Products Purchased v. Reviews Left")
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/5e135127a5d884c50a92aee835546784b2364b9a.png]]

We note that users who left the largest number of reviews bought somewhere under 10,000 games.

We consider only a handful of users, preferring those who left a large number of reviews.
#+begin_src jupyter-python :results none
  def get_reduced_users(users):
      users_sorted_by_reviews = users.sort_values("reviews", ascending=False)
      LIMIT = 40000
      return users_sorted_by_reviews.head(LIMIT)

  users = get_reduced_users(users)
#+end_src

Finally, we write the dataset into a new file,
#+begin_src jupyter-python :results none
  users.to_csv("users_processed.csv", index=False)
#+end_src

*** Recommendations
#+begin_src jupyter-python
  recommendations = pd.read_csv("recommendations.csv", index_col="review_id")
  print(recommendations.head())
  print(f"\n{recommendations.shape[0]} Entries")
#+end_src
#+RESULTS:
:             app_id  helpful  funny        date  is_recommended  hours  user_id
: review_id                                                                     
: 0           975370        0      0  2022-12-12            True   36.3    51580
: 1           304390        4      0  2017-02-17           False   11.5     2586
: 2          1085660        2      0  2019-11-17            True  336.5   253880
: 3           703080        0      0  2022-09-23            True   27.4   259432
: 4           526870        0      0  2021-01-10            True    7.9    23869
: 
: 41154794 Entries

The =funny= and =date= columns do not provide any useful information, so we drop them.
We also drop the =helpful= column since we are only interested in user similarities.
#+begin_src jupyter-python :results none
  recommendations = recommendations.drop(["funny", "date", "helpful"], axis=1)
#+end_src

We limit this dataset to contain only those games and users that we selected.
#+begin_src jupyter-python
  recommendations = recommendations[
      recommendations["app_id"].isin(games["app_id"])
      & recommendations["user_id"].isin(users["user_id"])
  ]

  print(f"{recommendations.shape[0]} Entries")
#+end_src
#+RESULTS:
: 1771884 Entries

Since the games and users were filtered out indepedant of each other, we can not say for sure that each user
left as many reviews as reported in the users dataset. So, we drop off the users with a low review count.
#+begin_src jupyter-python
  recommendations = recommendations.groupby("user_id").filter(lambda x: len(x) > 50)

  print(f"{recommendations.shape[0]} Entries")
#+end_src
#+RESULTS:
: 919920 Entries
/The =users= dataset will not be used further, so no point in updating it./
/The =games= dataset will be used for content based filtering, and can be used independant of the
recommendations dataset/

Saving this file,
#+begin_src jupyter-python :results none
  recommendations.to_csv("recommendations_processed.csv", index=False)
#+end_src

** The Model
:PROPERTIES:
:header-args:jupyter-python: :tangle game_recommender_model.py :session model
:END:

#+begin_src jupyter-python :results none :noweb yes
  <<common_imports>>
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import LabelEncoder
  from sklearn.metrics import classification_report
#+end_src

First, we make sure we are using the newly processed datasets.
#+begin_src jupyter-python :results none
  games = pd.read_csv("games_processed.csv")

  with open("games_metadata_processed.json") as source:
      games_metadata = pd.read_json(source)

  recommendations = pd.read_csv("recommendations_processed.csv")
#+end_src

Now, the following data is available to us:
#+begin_src jupyter-python
  print(f"Games: {list(games.keys())}")
  print(f"GamesMetadata: {list(games_metadata.keys())}")
  print(f"Recommendations: {list(recommendations.keys())}")
#+end_src
#+RESULTS:
: Games: ['app_id', 'title', 'rating', 'price_final']
: GamesMetadata: ['app_id', 'tags']
: Recommendations: ['app_id', 'is_recommended', 'hours', 'user_id']

The idea is to use the features of the games played by a user to test if they'd like an arbitrary game they
haven't yet played. This is equivalent to classifying a new game into one of ("will like", "won't like").
The features we shall use are
  - Overall rating of the game (as it appears on the =steam= store)
  - The price
  - The games' genres/tags (this makes use of the =is_recommended= feature from the =recommendations= dataset to
    select the tags preferred by the user)

*** Some More Processing

**** Grouping the Game Prices
We shall group the game prices into
  0. Free
  1. Budget :: if its under $50
  2. Expensive
#+begin_src jupyter-python :results none
  def classify_price(price: float) -> int:
      return (
          0 if price == 0
          else 1 if price < 50
          else 2
      )
#+end_src

After classifying the prices, we can drop the original =price_final= feature.
#+begin_src jupyter-python :results none
  games = games.assign(
      price_category=games["price_final"].apply(classify_price)
  )

  games = games.drop("price_final", axis=1)
#+end_src

**** Encode the Overall Rating
#+begin_src jupyter-python :results none
  games["rating_encoded"] = LabelEncoder().fit_transform(games["rating"])
  games = games.drop("rating", axis=1)
#+end_src

**** Unpacking the Genre information
It would be hard to compare genre lists, so we unpack them into binary columns of their own.
#+begin_src jupyter-python
  games_metadata["tags"] = games_metadata["tags"].apply(
      lambda tags: tags if isinstance(tags, list) else []
  )
  all_tags = set(tag for tags in games_metadata["tags"] for tag in tags)

  f"{len(all_tags)} tags in total"
#+end_src
#+RESULTS:
: 440 tags in total

Now, there are a lot of tags, so we keep only the more commonly recurring ones,
#+begin_src jupyter-python
  def plot_genre_distribution():
      THRESHOLD = 5000
      tags = games_metadata["tags"].explode()
      tag_counts = tags.value_counts()
      valid_tags = tag_counts[tag_counts >= THRESHOLD].keys()
      tags = tags[tags.isin(valid_tags)].value_counts()
      tags.plot.pie(autopct="%1.1f%%")
      plt.show()

  plot_genre_distribution()
#+end_src
#+RESULTS:
[[file:./.ob-jupyter/cf1958d54a600d2c6d04712ecc0c074a27eaf572.png]]
Apparently, Indie games make up most of the steam library.

#+begin_src jupyter-python
  def get_reduced_tags(all_tags):
      MIN = 2000
      tag_counts = games_metadata["tags"].explode().value_counts()
      return set(tag for tag in all_tags
                 if tag in tag_counts[tag_counts > MIN].keys())

  all_tags = get_reduced_tags(all_tags)
  f"{len(all_tags)} tags"
#+end_src
#+RESULTS:
: 40 tags

#+begin_src jupyter-python :results none
  for tag in all_tags:
      games_metadata[tag] = games_metadata["tags"].apply(lambda tags: int(tag in tags))

  games_metadata = games_metadata.drop("tags", axis=1)
#+end_src


*** Merging the Datasets
#+begin_src jupyter-python :results none
  games_unified = games.merge(games_metadata, how="left", on="app_id")
  df = recommendations.merge(games_unified, how="left", on="app_id")
#+end_src

*** Training and Testing
Now that we have a single unified dataset, we can use it for training our model.
The =app_id= provides us no more information than the game tags, rating and price, so we drop it, along with some other /useless/ information.
#+begin_src jupyter-python :results none
  X = df.drop(["app_id", "title", "is_recommended", "hours"], axis=1)
  y = df["is_recommended"]

  X_test, X_train, y_test, y_train = train_test_split(X, y, test_size=0.2, random_state=42)

  model = RandomForestClassifier(n_estimators=100, random_state=42)
  model.fit(X_train, y_train)
#+end_src

We then evaluate the model using it's precision, recall, and F1 scores.
#+begin_src jupyter-python
  y_pred = model.predict(X_test)
  print(classification_report(y_test, y_pred))
#+end_src
#+RESULTS:
:               precision    recall  f1-score   support
: 
:        False       0.32      0.29      0.30    160779
:         True       0.81      0.83      0.82    575157
: 
:     accuracy                           0.71    735936
:    macro avg       0.56      0.56      0.56    735936
: weighted avg       0.70      0.71      0.70    735936
: 

This model performs very poorly at answering if the user would dislike a game.
Our purpose for this model is, however, to recommend games that the user *might* like, which it does fairly well.
