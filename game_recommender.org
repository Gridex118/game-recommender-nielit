#+Author: Alex Rosegrid
#+Title: Game Recommender System as a Classification Problem
#+Startup: indent
#+PROPERTY: header-args:jupyter-python :session py :tangle games_recommender.py
#+OPTIONS: ^:nil


* Org, Jupyter Compatibility
First, activate the environment that has a jupyter installation. Then load the =emacs/jupyter= package.
#+begin_src emacs-lisp :results none
  (myPython/activate-conda-env "~/.opt/miniconda3/envs/nielit-project")
#+end_src
This can be used to convert the =org= file into a =ipynb= file.
#+begin_src shell :results none
  pandoc game_recommender.org -o game_recommender.ipynb
#+end_src


* The Report

** The Problem Statement
#+begin_quote
Given a list of games previously played by a user, recommend new games.
#+end_quote

** Imports
#+begin_src jupyter-python :results none
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns
  import json
#+end_src

** The Data Set
The dataset was obtained from [[https://www.kaggle.com/datasets/antonkozyriev/game-recommendations-on-steam?select=games.csv][Kaggle]]. It comprises of 4 files,
  - games.csv :: A record of games. Here, we are mainly interested in the title, ratings, and the final
    price.
  - games_metadata.json :: Provides the tags/genres for the games. This can be used to find similarities
    between games.
  - users.csv :: A list of users, the number of games they bought, and the number of times they reviewed
    anything. The number of games bought can be used to filter out a number of users from being considered.
  - recommendations.csv :: List of user reviews, in a form that answers if said user recommends a certain
    game or not.

** Data Cleaning
It is worth noting that these files, =users= and =recommendations= in particular, are much too large. Quite a
few fields are of no use to us.

*** Games
#+begin_src jupyter-python
  games = pd.read_csv("games.csv")
  print(games.head())
#+end_src

#+RESULTS:
#+begin_example
     app_id                              title date_release   win    mac  linux  \
  0   13500  Prince of Persia: Warrior Within™   2008-11-21  True  False  False   
  1   22364            BRINK: Agents of Change   2011-08-03  True  False  False   
  2  113020       Monaco: What's Yours Is Mine   2013-04-24  True   True   True   
  3  226560                 Escape Dead Island   2014-11-18  True  False  False   
  4  249050            Dungeon of the ENDLESS™   2014-10-27  True   True  False   

            rating  positive_ratio  user_reviews  price_final  price_original  \
  0  Very Positive              84          2199         9.99            9.99   
  1       Positive              85            21         2.99            2.99   
  2  Very Positive              92          3722        14.99           14.99   
  3          Mixed              61           873        14.99           14.99   
  4  Very Positive              88          8784        11.99           11.99   

     discount  steam_deck  
  0       0.0        True  
  1       0.0        True  
  2       0.0        True  
  3       0.0        True  
  4       0.0        True  
#+end_example

We are not concerned with the release dates, discounts or the original prices, only the final price of the
game; we can safely drop the corresponding fields.
#+begin_src jupyter-python :results none
  games = games.drop(["date_release", "price_original", "discount"], axis=1)
#+end_src

It's pobably a lot more efficient to just target a single platform, so let's choose the one with the largest
number of games compatible.
#+begin_src jupyter-python
  def plot_games_in_platform():
      platforms: list[str] = ["win", "steam_deck", "mac", "linux"]
      games_in_platform = games[platforms].sum()
      plt.bar(games_in_platform.keys(), games_in_platform, color=sns.color_palette("pastel"))
      plt.title("Number of games compatible with each platform")
      plt.xlabel("Platform")
      plt.ylabel("Compatible games")
      plt.show()

  plot_games_in_platform()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/33989c75f87303d0b891bbd95e72fea1283788f1.png]]

We note that the Stem Deck has the best compatibility, so, for simplicity's sake we assume that every user
owns one, and we can drop the other 3.
#+begin_src jupyter-python :results none
  games = games.drop(["win", "linux", "mac"], axis=1)
#+end_src
We also drop the games that are not compatible with the Steam Deck.
#+begin_src jupyter-python :results none
  games["steam_deck"] = games["steam_deck"].replace(False, np.nan)
  games = games.dropna(subset=["steam_deck"])
#+end_src

This leaves us with,
#+begin_src jupyter-python
  remaining_game_entries = games.shape[0]
  f"{remaining_game_entries} Entries"
#+end_src
#+RESULTS:
: 50870 Entries
Which is still very large, so we discard 30% of it
#+begin_src jupyter-python
  games = games.head(int(remaining_game_entries * 70 / 100))
  f"{games.shape[0]} Entries"
#+end_src
#+RESULTS:
: 35609 Entries

Finally, we write this file to disk for future use.
#+begin_src jupyter-python :results none
  games.to_csv("games_processed.csv", index=False)
#+end_src

*** Game Metadata
The original metadata file was badly formatted (set of top-level objects), so we use =jq= to fix
it. Additionally, we drop the game description.
#+begin_src shell :results none
  jq -s 'del(.[].description)' games_metadata.json > games_metadata_slurp.json
#+end_src

Now, remove the entries that are no longer present in the games dataset, and write this to a new file.
#+begin_src jupyter-python :results none
  with open("games_metadata_slurp.json") as source:
      games_metadata = json.load(source)
  games_metadata_filtered = [item for item in games_metadata
                             if item.get("app_id") in list(games["app_id"])]
  with open("games_metadata_processed.json", "w") as sink:
      json.dump(games_metadata_filtered, sink)
#+end_src

#+begin_src shell :results output verbatim
  printf "%s Entries" $(jq '.[].app_id' games_metadata_processed.json| wc -l)
#+end_src
#+RESULTS:
: 35609 Entries

*** Users
#+begin_src jupyter-python
  users = pd.read_csv("users.csv")
  print(users.head())
  print(f"\n{users.shape[0]} rows")
#+end_src

#+RESULTS:
:     user_id  products
: 0   7360263       359
: 1  14020781       156
: 2   8762579       329
: 3   4820647       176
: 4   5167327        98
: 
: 14306064 rows

The number of reviews given by the user is unnecessary data for our purposes - dropping.
#+begin_src jupyter-python :results none
  users = users.drop("reviews", axis=1)
#+end_src

Inspecting the dataset, a number of users never bought any products,
#+begin_src jupyter-python
  users["products"] = users["products"].replace(0, np.nan)
  users["products"].isna().sum()
#+end_src
#+RESULTS:
: np.int64(139318)
We can ignore them since they do not help in making any predictions.
#+begin_src jupyter-python
  users = users.dropna(subset=["products"])
  print(f"{users.shape[0]} rows")
#+end_src
#+RESULTS:
: 14166746 rows
This is still a very large dataset, so we select only a handful of users that bought a lot of products.
#+begin_src jupyter-python :results none
  def get_reduced_users(users):
      users_sorted_by_products = users.sort_values("products", ascending=False)
      LIMIT = 100000
      return users_sorted_by_products.head(LIMIT)

  users = get_reduced_users(users)
#+end_src

Finally, we write the dataset into a new file,
#+begin_src jupyter-python :results none
  users.to_csv("users_processed.csv", index=False)
#+end_src

